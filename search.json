[{"title":"爬取拉勾网的爬虫相关职位的数据","url":"/2019/05/30/爬取拉勾网的爬虫相关职位的数据/","content":"\n用requests请求拉勾网，并将爬取到的数据存储到MongoDB中。\n\n<!--more-->\n\n# 分析\n\n​\t首先，我想爬取的是与爬虫相关的职位，所以我在拉勾上直接搜索爬虫。\n\n​\t连续翻页，可以发现，拉勾网的网址url是不变的，然后可以使用浏览器，禁用Javascript资源的加载，重新刷新网页，发现网页的内容是无法加载出来的。这就更印证了拉勾网的前端数据是使用Ajex来加载的。\n\n​\t然后使用chorme浏览器的检查功能，如下图，发现网页返回的多是json格式的数据。\n\n![用chorme分析的结果](http://ww1.sinaimg.cn/large/006tNc79gy1g3jn9xgafij319n0u0dvr.jpg)\n\n​\t再查看请求信息，发现是post请求，请求的数据如下图，且pn为页数，kd为搜索的关键字。并且请求的url是https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false。点击进去是禁止访问的。\n\n![post请求的数据](http://ww2.sinaimg.cn/large/006tNc79gy1g3jndqxmphj30io0600t0.jpg)\n\n​\t再分析请求头，拉勾网除了一般的都需要的Cookie和User-Agent外还有一个特别的条件就是Referer。\n\n![Request Headers](http://ww2.sinaimg.cn/large/006tNc79gy1g3jng7yin7j31mq0qun8i.jpg)\n\n一顿分析完之后就可以开始构建爬虫了。\n\n# 构造爬虫\n\n​\t我一开始爬的时候直接用requests带上ua、cookie和referer的请求头去请求搜索页。结果返回的结果不是想要的。\n\n![错误的请求结果](http://ww3.sinaimg.cn/large/006tNc79gy1g3jnlg16bxj31ng0u043i.jpg)\n\n​\t然后问了问大佬，是拉勾的反爬机制中会验证cookie，直接用搜索后的cookie是不行的，然后我就拿拉勾网搜索首页的cookie来请求，就请求成功了。大家可以自己试试。\n\n​\t好了，既然可以成功请求并且返回网页数据了，就可以开始分析网页的结构来解析出想要的数据了。\n\n​\t拉钩网搜索页的结构很简单，是json格式的数据，职位的信息是在content下的positionResult下的result里面。\n\n​\t这里使用python的好处就来了，python视json格式的数据为字典dict类型的数据。处理起来也就跟处理字典类型的数据一样。所以直接对返回的json格式按键值取出数据即可。\n\n![按键值取出数据](http://ww3.sinaimg.cn/large/006tNc79gy1g3jnwl0oynj30r801ut8s.jpg)\n\n​\t这样就可以爬取出一页的职位信息了。\n\n\n\n​\t我们想要爬取的多页的数据，我们就来写一个结构化的爬虫，用一个循环来爬取多页的数据。并且可以将爬到的数据存在mongoDB中了。\n\n​\t使用mongodb只需要几条命令就可以了。\n\n![调用mongodb](http://ww3.sinaimg.cn/large/006tNc79gy1g3jo3bytwxj30is078q3d.jpg)\n\n​\t首先导入MongoClient；然后实例化client一个对象。client对象用来指向我们想要连接的数据库。然后指向一个表，mongodb里表叫collection。后面就可以通过insert来插入数据了。\n\n​\t我的爬虫是这样的，一开始想要先爬十页的数据，可是怕了五页就停了。大家可以自己试试。\n\n![](http://ww4.sinaimg.cn/large/006tNc79gy1g3jo8yqvdjj31r80tun2v.jpg)\n\n​\t然后我想了想，应该是拉钩网的反爬机制限制同一个cookie来短时间内多次请求。既然这样，我可以每爬一页前用requests获取一个新的cookie来请求。\n\n​\t因为需要用搜索首页的cookie所以就用requests请求搜索首页，获取到的cookie放到请求搜索页中的请求中。\n\n![](http://ww1.sinaimg.cn/large/006tNc79gy1g3jp0mflymj31hm0u0tg2.jpg)\n\n​\t这样每爬一页都用新的cookie就不怕cookie过期了。\n\n\n\n​\t可是～～～愉快的爬了几个职位信息之后，发现我的ip被禁了，现在的网站反爬太多了。这里就需要换新的ip来爬了，我们可以构建自己的ip池，当ip被禁了之后，从IP池中取出一个新的ip来爬取网页。这部分后面再说，今天先这样啦～掰掰","tags":["爬虫"],"categories":["爬虫"]},{"title":"测试文章","url":"/2019/05/27/测试文章/","content":"\n这是一篇测试文章，欢迎关注我的博客: https://LeuRutao.github.io/\n\n","tags":["介绍"],"categories":["测试"]},{"title":"Hello World","url":"/2019/05/27/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!--more-->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]