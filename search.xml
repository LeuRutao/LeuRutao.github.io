<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爱思益AceOffer用户体验感受]]></title>
    <url>%2F2019%2F06%2F17%2F%E7%88%B1%E6%80%9D%E7%9B%8AAceOffer%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%84%9F%E5%8F%97%2F</url>
    <content type="text"><![CDATA[第一天在爱思益AceOffer学习的一些感受。 背景​ 今天，在爱思益上报名了一个XXXX职位的《闪电计划》，初衷是希望能对该职位有更多的了解并学点基础职能，为秋招做点准备。 ​ 不过，经过在爱思益上学习了一天，课程内容还算不错，但是体验上感觉不太舒服，有些地方不得不吐槽一下。 不同平台的进入接口​ 最开始我是使用微信报名的，课程老师让我关注公众号，在公众号上进行注册。点击公众号的”学员中心”菜单下的子菜单”课程中心”，会跳转到一个H5网页。 点击H5网页中的菜单”我的”会跳转到登录页，点击注册跳转到注册页。 这两个页面的排布设计都是中规中矩，没有什么问题。 注册成功后，登录进去，到了个人中心页。 因为课程学习是需要通过观看视频，我觉得手机屏幕太小了，后面就用PC端浏览器看视频了。对比一下爱思益PC端网页的个人中心。 基本菜单都是一样的，不过微信端的个人中心让我看起来更舒服一点，更有设计感，PC端我个人认为就只是简单的功能排列而已。而且微信端中还有一个推荐奖励功能，但是点进去并没有详细内容，应该是一个还在开发的功能。 接下来是吐槽了点击我的课程后，会显示你所参加的全部课程。 继续点击进入学习，……这个章节展示页面，挺简约的。。。但是你能想象，每个视频连个名字都没有吗。我完全无法提前知道这节课这个视频是讲什么内容，第一次看还好，如果之后我要重新回看的话，我根本就没法知道我想要知道的内容在哪个视频，难道要用户自己记住每个视频讲什么内容，然后还要记住顺序吗？ 最让我想吐槽的是下面这个视频播放页！！！这个视频窗口占了整个网页整整90%还要多，我的电脑是13.3英寸的，看上去满满的压迫感。而且我需要拉滚动条才能看到暂停键！！ 不知道是录制的原因还是剪辑的原因，视频的声音非常小，我把电脑的声音开到最大了才刚刚能比较好的听清楚。而且老师讲得特别快，我怀疑是后期剪辑的时候调快了，但是你好歹也添加一个倍速的功能啊！！语速那么快，还不配字幕，还不能调倍速！！有几句话反反复复听了好多遍才听清楚。 好，没有字幕，不能调倍速，那我自己多听几遍吧。但是！居然不能空格键暂停！居然不能用键盘的左右键控制进度条！我的天！我每次要把视频退回一点重新听，我需要先点鼠标按暂停，然后拖动进度条到我要重新听的位置！！！这也太麻烦了吧！！！ 还有就是这个视频播放完了不会自动跳到下一个视频，每次都要回到我的课程页重新点开一个视频，有时视频加载得还贼慢！这里我想提个建议视频窗口占页面的2/3大小即可，如红色方框所示，绿色方框建议用来显示章节和视频列表。这样用户就不需要重新回到上一个页面来点击下一个视频了。 还有一个槽点，每节课都是有配备作业的，但是章节显示中并有没作业的快捷跳转入口，而是要点击我的作业，才能看到节课的作业。 最后提交作业时，文本最多不能超过2000字，而且只能上传6张图片，而且只能输入纯文本，表格等其他都没法做。这里我建议增加上传附件的功能，支持上传word或者pdf。这样就可以文本和图片一起提交了，需要做分析的时候，画个表格或者画个流程图也是必要的。 好了，这就是我第一天在爱思益上学习的体验，以上内容纯属个人观点，如有得罪，请多包涵！谢谢！]]></content>
      <categories>
        <category>用户体验</category>
      </categories>
      <tags>
        <tag>产品</tag>
        <tag>用户体验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取拉勾网的爬虫相关职位的数据]]></title>
    <url>%2F2019%2F05%2F30%2F%E7%88%AC%E5%8F%96%E6%8B%89%E5%8B%BE%E7%BD%91%E7%9A%84%E7%88%AC%E8%99%AB%E7%9B%B8%E5%85%B3%E8%81%8C%E4%BD%8D%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[用requests请求拉勾网，并将爬取到的数据存储到MongoDB中。 分析​ 首先，我想爬取的是与爬虫相关的职位，所以我在拉勾上直接搜索爬虫。 ​ 连续翻页，可以发现，拉勾网的网址url是不变的，然后可以使用浏览器，禁用Javascript资源的加载，重新刷新网页，发现网页的内容是无法加载出来的。这就更印证了拉勾网的前端数据是使用Ajex来加载的。 ​ 然后使用chorme浏览器的检查功能，如下图，发现网页返回的多是json格式的数据。 ​ 再查看请求信息，发现是post请求，请求的数据如下图，且pn为页数，kd为搜索的关键字。并且请求的url是https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false。点击进去是禁止访问的。 ​ 再分析请求头，拉勾网除了一般的都需要的Cookie和User-Agent外还有一个特别的条件就是Referer。 一顿分析完之后就可以开始构建爬虫了。 构造爬虫​ 我一开始爬的时候直接用requests带上ua、cookie和referer的请求头去请求搜索页。结果返回的结果不是想要的。 ​ 然后问了问大佬，是拉勾的反爬机制中会验证cookie，直接用搜索后的cookie是不行的，然后我就拿拉勾网搜索首页的cookie来请求，就请求成功了。大家可以自己试试。 ​ 好了，既然可以成功请求并且返回网页数据了，就可以开始分析网页的结构来解析出想要的数据了。 ​ 拉钩网搜索页的结构很简单，是json格式的数据，职位的信息是在content下的positionResult下的result里面。 ​ 这里使用python的好处就来了，python视json格式的数据为字典dict类型的数据。处理起来也就跟处理字典类型的数据一样。所以直接对返回的json格式按键值取出数据即可。 ​ 这样就可以爬取出一页的职位信息了。 ​ 我们想要爬取的多页的数据，我们就来写一个结构化的爬虫，用一个循环来爬取多页的数据。并且可以将爬到的数据存在mongoDB中了。 ​ 使用mongodb只需要几条命令就可以了。 ​ 首先导入MongoClient；然后实例化client一个对象。client对象用来指向我们想要连接的数据库。然后指向一个表，mongodb里表叫collection。后面就可以通过insert来插入数据了。 ​ 我的爬虫是这样的，一开始想要先爬十页的数据，可是怕了五页就停了。大家可以自己试试。 ​ 然后我想了想，应该是拉钩网的反爬机制限制同一个cookie来短时间内多次请求。既然这样，我可以每爬一页前用requests获取一个新的cookie来请求。 ​ 因为需要用搜索首页的cookie所以就用requests请求搜索首页，获取到的cookie放到请求搜索页中的请求中。 ​ 这样每爬一页都用新的cookie就不怕cookie过期了。 ​ 可是～～～愉快的爬了几个职位信息之后，发现我的ip被禁了，现在的网站反爬太多了。这里就需要换新的ip来爬了，我们可以构建自己的ip池，当ip被禁了之后，从IP池中取出一个新的ip来爬取网页。这部分后面再说，今天先这样啦～掰掰]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试文章]]></title>
    <url>%2F2019%2F05%2F27%2F%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[这是一篇测试文章，欢迎关注我的博客: https://LeuRutao.github.io/]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>介绍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
